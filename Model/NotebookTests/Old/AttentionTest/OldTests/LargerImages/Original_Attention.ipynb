{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Aug With No Test Aug, Basic Attention Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Parameters\n",
    "Here you can change some of the main parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "show_images = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import ops\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "sys.path.append(r'D:\\Estudos\\IC\\CNN\\CNN\\Model')\n",
    "from TrainingDataArrangement import arrange_data\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import CustomDataset\n",
    "from UNetModel import UNet\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "#defining device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):  \n",
    "    random.seed(seed) #setting for albumentations\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(727)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting path references\n",
    "testing_folders = [\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009009\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009010x1000902\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009011\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009014\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009016x1000903\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009023\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009009\\02-non_roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009010x1000902\\02-non_roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009011\\02-non_roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009014\\02-non_roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009016x1000903\\02-non_roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\testing\\tumor\\patch\\640x640\\1009023\\02-non_roi\"\n",
    "]\n",
    "\n",
    "training_folders = [\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009009\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009010x1000902\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009011\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009014\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009016x1000903\\01-roi\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009023\\01-roi\",\n",
    "    # r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009009\\02-non_roi\",\n",
    "    # r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009010x1000902\\02-non_roi\",\n",
    "    # r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009011\\02-non_roi\",\n",
    "    # r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009014\\02-non_roi\",\n",
    "    # r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009016x1000903\\02-non_roi\",\n",
    "    # r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\training\\tumor\\patch\\640x640\\1009023\\02-non_roi\"\n",
    "]\n",
    "\n",
    "updated_training_folders = []\n",
    "updated_testing_folders = []\n",
    "\n",
    "for training_folder, testing_folder in zip(training_folders, testing_folders):\n",
    "    original_training_folder = training_folder + \"\\\\01-original\"\n",
    "    mask_training_folder = training_folder + \"\\\\02-mask\"\n",
    "    updated_training_folders.extend([original_training_folder, mask_training_folder])\n",
    "\n",
    "    original_testing_folder = testing_folder + \"\\\\01-original\"\n",
    "    mask_testing_folder = testing_folder + \"\\\\02-mask\"\n",
    "    updated_testing_folders.extend([original_testing_folder, mask_testing_folder])\n",
    "\n",
    "training_path = r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\training\"\n",
    "test_path = r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\"\n",
    "\n",
    "arrange_data(test_path,updated_testing_folders)\n",
    "arrange_data(training_path,updated_training_folders)\n",
    "\n",
    "#data augmentation\n",
    "augmentation_transform = A.Compose([\n",
    "    #A.ToPIL(),\n",
    "    A.Resize(width = width, height = height, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.25),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    A.Transpose(p=0.25),\n",
    "    #A.RandomResizedCrop(224, 224, scale=(0.8, 1.0)),\n",
    "    # A.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), interpolation=1, border_mode=4, p=0.3),\n",
    "    # A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, interpolation=1, border_mode=4, p=0.3),\n",
    "    # A.OpticalDistortion(distort_limit=(-0.05, 0.05), shift_limit=(-0.05, 0.05), interpolation=1, border_mode=4, p=0.3),\n",
    "    # A.Perspective(scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1, p=0.3),\n",
    "    # A.PiecewiseAffine (scale=(0.03, 0.05), nb_rows=4, nb_cols=4, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode='constant', absolute_scale=False, keypoints_threshold=0.01, p=0.1),\n",
    "    # A.ShiftScaleRotate (shift_limit=(-0.0625, 0.0625), scale_limit=(-0.1, 0.1), rotate_limit=(-45, 45), interpolation=1, border_mode=4, value=0, mask_value=0, rotate_method='largest_box', p=0.15),\n",
    "    ToTensorV2()  # loading image to tensor and normalization\n",
    "])\n",
    "test_transform = A.Compose([\n",
    "    #A.ToPIL(),\n",
    "    A.Resize(width = width, height = height, p=1.0),\n",
    "    #A.HorizontalFlip(p=0.5),\n",
    "    #A.VerticalFlip(p=0.25),\n",
    "    #A.RandomRotate90(p=0.5),\n",
    "    #A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    #A.Transpose(p=0.5),\n",
    "    #A.RandomResizedCrop(224, 224, scale=(0.8, 1.0)),\n",
    "    ToTensorV2()  # loading image to tensor and normalization\n",
    "])\n",
    "\n",
    "print(\"Showing images:\", show_images)\n",
    "\n",
    "print(\"Getting the images\")\n",
    "training_dataset = CustomDataset.CustomImageDataset(training_path + \"\\\\images\", training_path + \"\\\\masks\", transform=augmentation_transform)\n",
    "test_dataset = CustomDataset.CustomImageDataset(test_path + \"\\\\images\", test_path + \"\\\\masks\", transform=test_transform)\n",
    "\n",
    "print(\"Converting images to tensors\")\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset , batch_size=batch_size, shuffle=False)\n",
    "if(show_images):\n",
    "    num_images_to_display = 10\n",
    "    fig, axs = plt.subplots(num_images_to_display, 2, figsize=(10, 20))\n",
    "    for i in range(num_images_to_display):\n",
    "        train_image, mask_image = training_dataset[i]\n",
    "        train_image = train_image.permute(1, 2, 0).numpy()\n",
    "\n",
    "        axs[i, 0].imshow(train_image)\n",
    "        axs[i, 0].set_title('Train Image')\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        mask_image = mask_image.squeeze(0).numpy()\n",
    "        axs[i, 1].imshow(mask_image, cmap='gray')\n",
    "        axs[i, 1].set_title('Mask Image')\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channels = 3\n",
    "classes = 1\n",
    "model = UNet(channels, classes, use_attention=True, attention_type=\"Basic_Attention\")\n",
    "input_data = torch.randn(1, channels, width, height)\n",
    "output = model(input_data)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = epochs\n",
    "\n",
    "training_accuracies = []\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "images_dir = \"epoch_images\"\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "for filename in os.listdir(images_dir): #deleting all previous images files\n",
    "    file_path = os.path.join(images_dir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "\n",
    "    for images, masks in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = F.binary_cross_entropy(outputs, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #thresholding the prediction values\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        binary_predictions = (outputs > 0.5).float()\n",
    "        \n",
    "        #calculating the correct pixels\n",
    "        correct_predictions += (binary_predictions == masks).sum().item()\n",
    "\n",
    "        total_samples += masks.numel()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    training_accuracies.append(epoch_accuracy)\n",
    "    training_losses.append(epoch_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy * 100:.2f}%\")\n",
    "\n",
    "    #validation\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0   \n",
    "    with torch.no_grad():\n",
    "        for val_images, val_masks in tqdm(test_dataloader, desc='Validation', leave=False):\n",
    "            val_images, val_masks = val_images.to(device), val_masks.to(device)\n",
    "            val_outputs = model(val_images)\n",
    "\n",
    "            val_loss += F.binary_cross_entropy(val_outputs, val_masks).item()\n",
    "\n",
    "            val_binary_predictions = (val_outputs > 0.5).float()\n",
    "\n",
    "            val_correct_predictions += (val_binary_predictions == val_masks).sum().item()\n",
    "\n",
    "            val_total_samples += val_masks.numel()\n",
    "\n",
    "    val_epoch_loss = val_loss / len(test_dataloader)\n",
    "    val_epoch_accuracy = val_correct_predictions / val_total_samples\n",
    "\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy * 100:.2f}%\")\n",
    "            \n",
    "\n",
    "    if(show_images):\n",
    "        with torch.no_grad():\n",
    "            for images, masks in test_dataloader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "                if torch.any(masks > 0):\n",
    "                    \n",
    "                    outputs = model(images)\n",
    "                    probs = outputs\n",
    "                    images_np = images.cpu().numpy()\n",
    "                    masks_np = masks.cpu().numpy()\n",
    "                    probs_np = probs.cpu().numpy()\n",
    "                    threshold = 0.5\n",
    "                    preds_np = (probs_np > threshold).astype(np.uint8)\n",
    "                    preds_np_squeezed = np.squeeze(preds_np, axis=1) \n",
    "                    plt.figure(figsize=(16, 4))\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.imshow(images_np[0].transpose((1, 2, 0)))\n",
    "                    plt.imshow(masks_np[0, 0], alpha=0.5, cmap='plasma')\n",
    "                    plt.title(\"Original Image with Ground Truth Mask\")\n",
    "                    plt.axis('off')\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.imshow(images_np[0].transpose((1, 2, 0)))\n",
    "                    plt.imshow(preds_np_squeezed[0], alpha=0.5, cmap='plasma')\n",
    "                    plt.title(\"Original Image with Predicted Mask\")\n",
    "                    plt.axis('off')\n",
    "                    plt.savefig(os.path.join(images_dir, f'epoch_{epoch + 1}.png'))\n",
    "                    if epoch % 5 == 0: plt.show()\n",
    "                    plt.close()\n",
    "                    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt \n",
    "# from pytorch_grad_cam import GradCAM\n",
    "# from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "# from pytorch_grad_cam.utils.model_targets import SemanticSegmentationTarget\n",
    "\n",
    "# model.set_grad_cam_target(model.upconv2)\n",
    "\n",
    "# # Example Grad-CAM code\n",
    "# target_layer = model.grad_cam_target\n",
    "# cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "# for images, masks in test_dataloader:\n",
    "#     images, masks = images.to(device), masks.to(device)\n",
    "#     outputs = model(images)\n",
    "\n",
    "#     target_category = outputs.argmax(dim=1)\n",
    "    \n",
    "#     h, w = outputs.shape[2] // 2, outputs.shape[3] // 2\n",
    "#     target_category_index = target_category[0, h, w].item()\n",
    "\n",
    "#     mask = torch.zeros_like(target_category[0], dtype=torch.float32)\n",
    "#     mask[h, w] = 1.0\n",
    "#     mask = mask.cpu().numpy()\n",
    "\n",
    "#     targets = [SemanticSegmentationTarget(target_category_index, mask)]\n",
    "\n",
    "#     heatmap = cam(input_tensor=images, targets=targets)\n",
    "#     gradCAMMap = heatmap[0]\n",
    "\n",
    "#     input_image = images[0].cpu().permute(1, 2, 0).numpy()\n",
    "#     cam_image = show_cam_on_image(input_image, gradCAMMap)\n",
    "\n",
    "#     # Convert mask tensor to numpy array and reshape if necessary\n",
    "#     mask_np = masks[0].cpu().numpy()\n",
    "#     if mask_np.ndim == 3:\n",
    "#         mask_np = mask_np[0]  # Assuming mask is (1, 256, 256), take the first channel\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "\n",
    "#     plt.subplot(2, 2, 1)\n",
    "#     plt.imshow(input_image)\n",
    "#     plt.title(\"Original Image\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.subplot(2, 2, 2)\n",
    "#     plt.imshow(mask_np, cmap='gray')  # Display the mask\n",
    "#     plt.title(\"Mask\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.subplot(2, 2, 3)\n",
    "#     plt.imshow(gradCAMMap, alpha=0.5, cmap='jet')\n",
    "#     plt.title(\"Grad-CAM Heatmap\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.subplot(2, 2, 4)\n",
    "#     plt.imshow(cam_image)\n",
    "#     plt.title(\"Grad-CAM Overlay\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "\n",
    "images = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    images.append(imageio.v2.imread(os.path.join(images_dir, f'epoch_{epoch}.png')))\n",
    "\n",
    "gif_path = os.path.join(images_dir, 'training_progress.gif')\n",
    "imageio.mimsave(gif_path, images, duration=1, loop=0)\n",
    "\n",
    "from IPython import display\n",
    "display.Image(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "- **Accuracy**: The number of correct predictions out of the total of predictions. Higher means better.\n",
    "\n",
    "- **Loss**: How well the predictions matched the expected result. Lower means better.\n",
    "\n",
    "- **Dice**: Calculates the similarity between the intersection of two images. Higher means better.\n",
    "\n",
    "- **Intersection over Union (IoU)**: It calculates the overlap between the predicted image and the ground truth, but it also takes in consideration the union of the boxes, while Dice only accounts for the intersection.Higher means better.\n",
    "\n",
    "- **Panoptic Quality (PQ)**: Indicates the performance in segmenting instances accurately. Higher means better.\n",
    "\n",
    "- **Aggregated Jaccard Index (AJI)**: Indicate the performance of the alignment between predicted and ground truth instance boundaries. Higher means better.\n",
    "\n",
    "- **Specificity**: Measures the proportion of actual negatives that are correctly identified. Higher means better.\n",
    "\n",
    "- **Sensitivity**: Measures the proportion of actual positives that are correctly identified. Higher means better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(predictions, targets):\n",
    "    true_positives = ((predictions == 1) & (targets == 1)).sum().item()\n",
    "    true_negatives = ((predictions == 0) & (targets == 0)).sum().item()\n",
    "    false_positives = ((predictions == 1) & (targets == 0)).sum().item()\n",
    "    false_negatives = ((predictions == 0) & (targets == 1)).sum().item()\n",
    "    \n",
    "    intersection = torch.logical_and(predictions, targets).sum().item()\n",
    "    union = torch.logical_or(predictions, targets).sum().item()\n",
    "    \n",
    "    return true_positives, true_negatives, false_positives, false_negatives, intersection, union\n",
    "\n",
    "model.eval()\n",
    "accuracies = []\n",
    "losses = []\n",
    "dices = [] \n",
    "ious = []\n",
    "specificities = []\n",
    "sensitivities = []\n",
    "pqs = []\n",
    "ajis = []\n",
    "\n",
    "ev_running_loss = 0.0\n",
    "ev_correct_predictions = 0\n",
    "ev_total_samples = 0\n",
    "ev_true_positives = 0\n",
    "ev_true_negatives = 0\n",
    "ev_false_positives = 0\n",
    "ev_false_negatives = 0\n",
    "ev_aji = 0\n",
    "batch_iou = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ev_images, ev_masks in test_dataloader:\n",
    "        ev_images, ev_masks = ev_images.to(device), ev_masks.to(device)\n",
    "        ev_outputs = model(ev_images)\n",
    "        ev_loss = F.binary_cross_entropy(ev_outputs, ev_masks)\n",
    "        ev_running_loss += ev_loss.item()\n",
    "        losses.append(ev_loss.item())\n",
    "\n",
    "        #Thresholding the prediction values\n",
    "        ev_binary_predictions = (ev_outputs > 0.5).float()\n",
    "\n",
    "        #Calculating the correct pixels\n",
    "        ev_correct_predictions += (ev_binary_predictions == ev_masks).sum().item()\n",
    "        ev_total_samples += ev_masks.numel()\n",
    "\n",
    "        #Calculate metrics\n",
    "        tp, tn, fp, fn, intersection, union = calculate_metrics(ev_binary_predictions, ev_masks)\n",
    "        \n",
    "        ev_true_positives += tp\n",
    "        ev_true_negatives += tn\n",
    "        ev_false_positives += fp\n",
    "        ev_false_negatives += fn\n",
    "        \n",
    "        #Calculating IoU\n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "        batch_iou += iou\n",
    "        ious.append(iou)\n",
    "\n",
    "        #Calculating AJI\n",
    "        aji_numerator = intersection\n",
    "        aji_denominator = union + fp + fn\n",
    "        aji = aji_numerator / aji_denominator if aji_denominator > 0 else 0.0\n",
    "        ev_aji += aji\n",
    "        ajis.append(aji)\n",
    "\n",
    "        #Calculate accuracy\n",
    "        batch_accuracy = (ev_binary_predictions == ev_masks).sum().item() / ev_masks.numel()\n",
    "        accuracies.append(batch_accuracy)\n",
    "\n",
    "        #Calculate Dice coefficient\n",
    "        dice_coefficient = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0\n",
    "        dices.append(dice_coefficient)\n",
    "\n",
    "        #Precision and recall\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        pqs.append(precision * recall if precision + recall > 0 else 0.0)\n",
    "        \n",
    "        #Specificity and sensitivity\n",
    "        sensitivities.append(recall)\n",
    "        specificities.append(tn / (tn + fp) if (tn + fp) > 0 else 0.0)\n",
    "\n",
    "#Precision and recall\n",
    "precision = ev_true_positives / (ev_true_positives + ev_false_positives) if (ev_true_positives + ev_false_positives) > 0 else 0.0\n",
    "recall = ev_true_positives / (ev_true_positives + ev_false_negatives) if (ev_true_positives + ev_false_negatives) > 0 else 0.0\n",
    "\n",
    "#Specificity and sensitivity\n",
    "ev_sensitivity = recall\n",
    "ev_specificity = ev_true_negatives / (ev_true_negatives + ev_false_positives) if (ev_true_negatives + ev_false_positives) > 0 else 0.0\n",
    "\n",
    "#Evaluation loss and accuracy\n",
    "ev_loss = ev_running_loss / len(test_dataloader)\n",
    "ev_accuracy = ev_correct_predictions / ev_total_samples\n",
    "\n",
    "#Dice coefficient\n",
    "ev_temp_dice = (2 * ev_true_positives) / (2 * ev_true_positives + ev_false_positives + ev_false_negatives) if (2 * ev_true_positives + ev_false_positives + ev_false_negatives) > 0 else 0.0\n",
    "\n",
    "#PQ\n",
    "ev_pq = precision * recall if precision + recall > 0 else 0.0\n",
    "\n",
    "#IoU\n",
    "iou = batch_iou / len(test_dataloader)\n",
    "\n",
    "#AJI\n",
    "aji = ev_aji / len(test_dataloader)\n",
    "\n",
    "#Calculate standard deviation\n",
    "loss_std = np.std(losses)\n",
    "accuracy_std = np.std(accuracies)\n",
    "dice_std = np.std(dices)\n",
    "iou_std = np.std(ious)\n",
    "specificity_std = np.std(specificities)\n",
    "sensitivity_std = np.std(sensitivities)\n",
    "pq_std = np.std(pqs)\n",
    "aji_std = np.std(ajis)\n",
    "\n",
    "print(f'Evaluation Loss: {ev_loss * 100:0.2f}% (Std: {loss_std * 100:0.2f}%)')\n",
    "print(f'Evaluation Accuracy: {ev_accuracy * 100:0.2f}% (Std: {accuracy_std * 100:0.2f}%)')\n",
    "print(f'Dice Coefficient: {ev_temp_dice * 100:0.2f}% (Std: {dice_std * 100:0.2f}%)')\n",
    "print(f'IoU: {iou * 100:0.2f}% (Std: {iou_std * 100:0.2f}%)')\n",
    "print(f'Specificity: {ev_specificity * 100:0.2f}% (Std: {specificity_std * 100:0.2f}%)')\n",
    "print(f'Sensitivity: {ev_sensitivity * 100:0.2f}% (Std: {sensitivity_std * 100:0.2f}%)')\n",
    "print(f'PQ: {ev_pq * 100:0.2f}% (Std: {pq_std * 100:0.2f}%)')\n",
    "print(f'AJI: {aji * 100:0.2f}% (Std: {aji_std * 100:0.2f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Charts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs_list, training_losses, label='Training Loss', color='blue')\n",
    "plt.plot(epochs_list, val_losses, label='Validation Loss', color='orange')\n",
    "plt.plot(epochs_list, training_accuracies, label='Training Accuracy', color='green')\n",
    "plt.plot(epochs_list, val_accuracies, label='Validation Accuracy', color='red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Training and Validation Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.patches as mpatches\n",
    "if show_images:\n",
    "    with torch.no_grad():\n",
    "        counter = 0\n",
    "        brightness = 0.5\n",
    "        for images, masks in test_dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            images_normalized = images\n",
    "\n",
    "            outputs = model(images_normalized)\n",
    "            probs = outputs\n",
    "            #probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            images_np = images_normalized.cpu().numpy()\n",
    "            masks_np = masks.cpu().numpy()\n",
    "            probs_np = probs.cpu().numpy()\n",
    "\n",
    "            threshold = 0.5\n",
    "            preds_np = (probs_np > threshold).astype(np.uint8)\n",
    "            preds_np_squeezed = np.squeeze(preds_np, axis=1) \n",
    "\n",
    "            original_image = images_np[0].transpose((1, 2, 0))\n",
    "            ground_truth_mask = masks_np[0, 0]\n",
    "            predicted_mask = preds_np_squeezed[0]\n",
    "            probability_map = probs_np[0, 0]\n",
    "\n",
    "            overlay_gt = original_image.copy()\n",
    "            overlay_gt = original_image * brightness # to make it a little darker\n",
    "            overlay_gt[ground_truth_mask == 1] = [1, 1, 0]\n",
    "\n",
    "            overlay_pred = original_image.copy()\n",
    "            overlay_pred = original_image * brightness\n",
    "            overlay_pred[predicted_mask == 1] = [0, 1, 1]\n",
    "\n",
    "            overlay_diff = original_image.copy()\n",
    "            overlay_diff = original_image * brightness \n",
    "            overlay_diff[(ground_truth_mask == 1) & (predicted_mask == 1)] = [0, 1, 0]  #correct predictions\n",
    "            overlay_diff[(ground_truth_mask == 1) & (predicted_mask == 0)] = [1, 0, 0]  #missed predictions\n",
    "            overlay_diff[(ground_truth_mask == 0) & (predicted_mask == 1)] = [0, 0, 1]  #false positives\n",
    "\n",
    "            plt.figure(figsize=(28, 4))\n",
    "\n",
    "            plt.subplot(1, 7, 1)\n",
    "            plt.imshow(original_image)\n",
    "            plt.title(\"Original Image\")\n",
    "\n",
    "            plt.subplot(1, 7, 2)\n",
    "            plt.imshow(ground_truth_mask, cmap='gray')\n",
    "            plt.title(\"Ground Truth Mask\")\n",
    "\n",
    "            plt.subplot(1, 7, 3)\n",
    "            plt.imshow(probability_map, cmap='plasma')\n",
    "            plt.title(\"Probability Map\")\n",
    "\n",
    "            plt.subplot(1, 7, 4)\n",
    "            plt.imshow(predicted_mask, cmap='gray')\n",
    "            plt.title(\"Predicted Mask\")\n",
    "\n",
    "            plt.subplot(1, 7, 5)\n",
    "            plt.imshow(overlay_gt)\n",
    "            plt.title(\"Overlay Ground Truth\")\n",
    "\n",
    "            plt.subplot(1, 7, 6)\n",
    "            plt.imshow(overlay_pred)\n",
    "            plt.title(\"Overlay Pred\")\n",
    "\n",
    "            green_patch = mpatches.Patch(color='green', label='True Positive')\n",
    "            red_patch = mpatches.Patch(color='red', label='False Negative')\n",
    "            blue_patch = mpatches.Patch(color='blue', label='False Positive')\n",
    "            plt.subplot(1, 7, 7)\n",
    "            plt.imshow(overlay_diff)\n",
    "            plt.title(\"Overlay Diff\")\n",
    "            plt.legend(handles=[green_patch, red_patch, blue_patch], loc='upper right')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            counter += 1\n",
    "            if counter == 10:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda.amp as amp\n",
    "\n",
    "gradcam_width = 256\n",
    "gradcam_height = 256\n",
    "\n",
    "def save_and_display_gradcam(img_path, heatmap, alpha=0.6):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (gradcam_width, gradcam_height))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    mask_path = img_path.replace(r\"\\testing\\images\", r\"\\testing\\masks\")\n",
    "    mask = cv2.imread(mask_path)\n",
    "    mask = cv2.resize(mask, (gradcam_width, gradcam_height))\n",
    "\n",
    "    superimposed_img = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('GradCAM')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, alpha=0.5)\n",
    "    plt.title(\"Original Mask With Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def make_gradcam_heatmap(img_tensor, model, target_layer):\n",
    "    model.eval()\n",
    "    \n",
    "    features = None\n",
    "    gradients = None\n",
    "    \n",
    "    def forward_hook(module, input, output):\n",
    "        nonlocal features\n",
    "        features = output\n",
    "    \n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        nonlocal gradients\n",
    "        gradients = grad_out[0]\n",
    "    \n",
    "    handle_forward = target_layer.register_forward_hook(forward_hook)\n",
    "    handle_backward = target_layer.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    with amp.autocast():\n",
    "        preds = model(img_tensor)\n",
    "        pred_class = preds.argmax(dim=1)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    class_score = preds[:, pred_class].squeeze().sum()\n",
    "    class_score.backward(retain_graph=True)\n",
    "    \n",
    "    handle_forward.remove()\n",
    "    handle_backward.remove()\n",
    "    \n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    for i in range(features.shape[1]):\n",
    "        features[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "    heatmap = features.mean(dim=1).squeeze()\n",
    "    heatmap = np.maximum(heatmap.detach().cpu().numpy(), 0)\n",
    "    heatmap = heatmap / np.max(heatmap)\n",
    "    \n",
    "    del features, gradients\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "def make_prediction_and_visualize(image_paths):\n",
    "    for img_path in image_paths:\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (gradcam_width, gradcam_height))\n",
    "        img_tensor = torch.tensor(img / 255.0).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "        target_layer = model.upconv2\n",
    "\n",
    "        heatmap = make_gradcam_heatmap(img_tensor.to(device), model, target_layer)\n",
    "\n",
    "        save_and_display_gradcam(img_path, heatmap)\n",
    "\n",
    "image_paths = [\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r27c5.png\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r27c10.png\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r28c12.png\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r31c0.png\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r31c1.png\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r35c4.png\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r35c8.png\",\n",
    "    r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r35c10.png\",\n",
    "]\n",
    "\n",
    "make_prediction_and_visualize(image_paths)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
