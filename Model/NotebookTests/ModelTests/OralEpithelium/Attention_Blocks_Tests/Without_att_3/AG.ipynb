{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Parameters\n",
    "Here you can change some of the main parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "epochs = 50\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "use_attention = True\n",
    "attention_model = \"Attention_Gate\"\n",
    "\n",
    "new_aug_epoch_distance = 5 #how many epochs between new augmentations\n",
    "number_of_augmentations = 3\n",
    "\n",
    "show_images = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENSLIDE_PATH = r'D:\\Estudos\\IC\\Libraries\\openslide-bin-4.0.0.3-windows-x64\\openslide-bin-4.0.0.3-windows-x64\\bin'\n",
    "\n",
    "import os\n",
    "if hasattr(os, 'add_dll_directory'):\n",
    "    with os.add_dll_directory(OPENSLIDE_PATH):\n",
    "        import openslide\n",
    "else:\n",
    "    import openslide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import softmax\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from PIL import Image\n",
    "from torchvision import ops\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as TF\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import imageio\n",
    "from IPython import display\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, GradCAMElementWise, GradCAMPlusPlus, XGradCAM, AblationCAM, ScoreCAM\n",
    "from pytorch_grad_cam import EigenCAM, EigenGradCAM, LayerCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "sys.path.append(r'D:\\Estudos\\IC\\CNN\\CNN\\Model')\n",
    "from TrainingDataArrangement import arrange_data\n",
    "import CustomDataset\n",
    "from UNetModel import UNet\n",
    "from DaliDataAug import load_pil_image, data_augmentation\n",
    "\n",
    "\n",
    "#defining device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):  \n",
    "    random.seed(seed) #setting for albumentations\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(727)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_path = r\"D:\\Estudos\\IC\\DataSets\\OralEpithelium\\UnetTraining\\Training\\Images\"\n",
    "mask_path = r\"D:\\Estudos\\IC\\DataSets\\OralEpithelium\\UnetTraining\\Training\\Masks\"\n",
    "\n",
    "#data augmentation\n",
    "augmentation_transform = A.Compose([\n",
    "    #A.ToPIL(),\n",
    "    A.Resize(width = width, height = height, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.25),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    A.Transpose(p=0.25),\n",
    "    #A.RandomResizedCrop(224, 224, scale=(0.8, 1.0)),\n",
    "    # A.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), interpolation=1, border_mode=4, p=0.3),\n",
    "    # A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, interpolation=1, border_mode=4, p=0.3),\n",
    "    # A.OpticalDistortion(distort_limit=(-0.05, 0.05), shift_limit=(-0.05, 0.05), interpolation=1, border_mode=4, p=0.3),\n",
    "    # A.Perspective(scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1, p=0.3),\n",
    "    # A.PiecewiseAffine (scale=(0.03, 0.05), nb_rows=4, nb_cols=4, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode='constant', absolute_scale=False, keypoints_threshold=0.01, p=0.1),\n",
    "    # A.ShiftScaleRotate (shift_limit=(-0.0625, 0.0625), scale_limit=(-0.1, 0.1), rotate_limit=(-45, 45), interpolation=1, border_mode=4, value=0, mask_value=0, rotate_method='largest_box', p=0.15),\n",
    "    ToTensorV2()  # loading image to tensor and normalization\n",
    "])\n",
    "test_transform = A.Compose([\n",
    "    #A.ToPIL(),\n",
    "    A.Resize(width = width, height = height, p=1.0),\n",
    "    #A.HorizontalFlip(p=0.5),\n",
    "    #A.VerticalFlip(p=0.25),\n",
    "    #A.RandomRotate90(p=0.5),\n",
    "    #A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    #A.Transpose(p=0.5),\n",
    "    #A.RandomResizedCrop(224, 224, scale=(0.8, 1.0)),\n",
    "    ToTensorV2()  # loading image to tensor and normalization\n",
    "])\n",
    "\n",
    "dataset = CustomDataset.CustomImageDataset(training_path, mask_path, transform=test_transform)\n",
    "\n",
    "# 70/30 split\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "print(\"Showing images:\", show_images)\n",
    "\n",
    "\n",
    "print(\"Converting images to tensors\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "if show_images:\n",
    "    num_images_to_display = 10\n",
    "    fig, axs = plt.subplots(num_images_to_display, 2, figsize=(10, 20))\n",
    "    for i in range(num_images_to_display):\n",
    "        train_image, mask_image, _ = train_dataset[i]\n",
    "        train_image = train_image.permute(1, 2, 0).numpy()\n",
    "\n",
    "        axs[i, 0].imshow(train_image)\n",
    "        axs[i, 0].set_title('Train Image')\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        mask_image = mask_image.squeeze(0).numpy()\n",
    "        axs[i, 1].imshow(mask_image, cmap='gray')\n",
    "        axs[i, 1].set_title('Mask Image')\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dir Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dir_checker_and_restarter(images_dir):\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    for filename in os.listdir(images_dir): #deleting all previous images files\n",
    "        file_path = os.path.join(images_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "def center_crop_image(image, width, height):\n",
    "    original_width, original_height = image.size\n",
    "\n",
    "    left = (original_width - width) / 2\n",
    "    top = (original_height - height) / 2\n",
    "    right = left + width\n",
    "    bottom = top + height\n",
    "\n",
    "    return image.crop((left, top, right, bottom))\n",
    "\n",
    "def save_image(image, path):\n",
    "    image.save(path)\n",
    "\n",
    "def recreate_data_loader(number_of_augmentations = 5):\n",
    "    input_dir_train = r\"D:\\Estudos\\IC\\DataSets\\OralEpithelium\\UnetTraining\\Training\"\n",
    "    output_dir_train = r\"D:\\Estudos\\IC\\DataSets\\OralEpithelium\\UnetTraining\\Training\\Transformed\"\n",
    "\n",
    "    main_dirs = [output_dir_train]\n",
    "\n",
    "    for dir in main_dirs:\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir + \"\\Images\")\n",
    "            os.makedirs(dir + \"\\Masks\")\n",
    "        else:\n",
    "            dir_checker_and_restarter(dir)\n",
    "\n",
    "    augmentation = [\"horizontal_flip\", \"vertical_flip\", \"rotation\", \"transpose\"]\n",
    "\n",
    "    img_input_size = (width, height)\n",
    "    img_output_size = (width, height)\n",
    "\n",
    "    input_output_mapping = {\n",
    "        \"Training\": output_dir_train,\n",
    "    }\n",
    "\n",
    "    dirs = [input_dir_train]\n",
    "    for i in range(number_of_augmentations):\n",
    "        for dir in dirs:\n",
    "            img_path_appended = os.path.join(dir, \"Images\")\n",
    "            mask_path_appended = os.path.join(dir, \"Masks\")\n",
    "\n",
    "            image_files = [f for f in os.listdir(img_path_appended) if f.endswith(('.jpg', '.png', '.jpeg', 'tif'))]\n",
    "            mask_files = [f for f in os.listdir(mask_path_appended) if f.endswith(('.jpg', '.png', '.jpeg', 'tif'))]\n",
    "\n",
    "            for img_name, mask_name in zip(image_files, mask_files):\n",
    "                img_path = os.path.join(img_path_appended, img_name)\n",
    "                mask_path = os.path.join(mask_path_appended, mask_name)\n",
    "\n",
    "                input_image = load_pil_image(img_path, False)\n",
    "                mask = load_pil_image(mask_path, False)\n",
    "                cropped_input_image = center_crop_image(input_image, width, height)\n",
    "                cropped_mask = center_crop_image(mask, width, height)\n",
    "\n",
    "                target_image = None\n",
    "                GAN_model = None\n",
    "\n",
    "                augmented_img, augmented_mask, used_augmentations = data_augmentation(cropped_input_image, target_image, cropped_mask, img_input_size, img_output_size, augmentation, GAN_model)\n",
    "\n",
    "                for key in input_output_mapping:\n",
    "                    if key in dir:\n",
    "                        output_dir = input_output_mapping[key]\n",
    "                        break\n",
    "                    \n",
    "                output_img_path = os.path.join(output_dir, f\"Images\\\\aug{i}_{img_name}\")\n",
    "                output_mask_path = os.path.join(output_dir, f\"Masks\\\\aug{i}_{mask_name}\")\n",
    "                save_image(TF.to_pil_image(augmented_img), output_img_path)\n",
    "                save_image(TF.to_pil_image(augmented_mask), output_mask_path)\n",
    "    print(f\"Finished loading {number_of_augmentations} new augmentations in the whole dataset\")\n",
    "                \n",
    "\n",
    "    # 70/30 split\n",
    "    training_dataset = CustomDataset.CustomImageDataset(output_dir_train + \"\\\\Images\", output_dir_train + \"\\\\Masks\", transform=test_transform)\n",
    "    train_size = int(0.7 * len(training_dataset))\n",
    "    test_size = len(training_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(training_dataset, [train_size, test_size])\n",
    "\n",
    "    test_dataset.dataset.transform = test_transform\n",
    "    training_dataset = train_dataset\n",
    "\n",
    "    train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset , batch_size=batch_size, shuffle=False)\n",
    "    return train_dataloader, test_dataloader\n",
    "train_dataloader, test_dataloader = recreate_data_loader(number_of_augmentations)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam_width = width\n",
    "gradcam_height = height\n",
    "\n",
    "def save_and_display_gradcam(img_path, heatmap, alpha=0.6):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (gradcam_width, gradcam_height))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    mask_path = img_path.replace(r\"\\testing\\images\", r\"\\testing\\masks\")\n",
    "    mask = cv2.imread(mask_path)\n",
    "    mask = cv2.resize(mask, (gradcam_width, gradcam_height))\n",
    "\n",
    "    superimposed_img = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('GradCAM')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, alpha=0.5)\n",
    "    plt.title(\"Original Mask With Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def make_gradcam_heatmap(img_tensor, model, target_layer):\n",
    "    model.eval()\n",
    "    \n",
    "    features = None\n",
    "    gradients = None\n",
    "    \n",
    "    def forward_hook(module, input, output):\n",
    "        nonlocal features\n",
    "        features = output\n",
    "    \n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        nonlocal gradients\n",
    "        gradients = grad_out[0]\n",
    "    \n",
    "    handle_forward = target_layer.register_forward_hook(forward_hook)\n",
    "    handle_backward = target_layer.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    with amp.autocast():\n",
    "        preds = model(img_tensor)\n",
    "        pred_class = preds.argmax(dim=1)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    class_score = preds[:, pred_class].squeeze().sum()\n",
    "    class_score.backward(retain_graph=True)\n",
    "    \n",
    "    handle_forward.remove()\n",
    "    handle_backward.remove()\n",
    "    \n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    for i in range(features.shape[1]):\n",
    "        features[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "    heatmap = features.mean(dim=1).squeeze()\n",
    "    heatmap = np.maximum(heatmap.detach().cpu().numpy(), 0)\n",
    "    epsilon = 1e-8\n",
    "    heatmap_max = np.max(heatmap)\n",
    "    heatmap = heatmap / (heatmap_max + epsilon)\n",
    "    \n",
    "    del features, gradients\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "def make_prediction_and_visualize(image_paths):\n",
    "    for img_path in image_paths:\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (gradcam_width, gradcam_height))\n",
    "        img_tensor = torch.tensor(img / 255.0).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "        target_layer = model.upconv2\n",
    "\n",
    "        heatmap = make_gradcam_heatmap(img_tensor.to(device), model, target_layer)\n",
    "\n",
    "        save_and_display_gradcam(img_path, heatmap)\n",
    "\n",
    "def generate_heatmap(img_path): \n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (gradcam_width, gradcam_height))\n",
    "    img_tensor = torch.tensor(img / 255.0).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    target_layer = model.upconv2\n",
    "    return make_gradcam_heatmap(img_tensor.to(device), model, target_layer)\n",
    "\n",
    "def generate_heatmap_with_tensor(img): \n",
    "    target_layer = model.upconv2\n",
    "    return make_gradcam_heatmap(img.unsqueeze(0), model, target_layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# image_paths = [\n",
    "#     r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r27c5.png\",\n",
    "#     r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r27c10.png\",\n",
    "#     r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r28c12.png\",\n",
    "#     r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r31c0.png\",\n",
    "#     r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r31c1.png\",\n",
    "#     r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r35c4.png\",\n",
    "#     r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r35c8.png\",\n",
    "#     r\"D:\\Estudos\\IC\\DataSets\\H&E-stained oral squamous cell carcinoma histological images dataset\\H&E-stained oral squamous cell carcinoma histological images dataset\\appended_folder\\testing\\images\\1009009_r35c10.png\",\n",
    "# ]\n",
    "\n",
    "# make_prediction_and_visualize(image_paths)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ResNet_CAM(model, image, index, cam_layer, cam_type = \"gradcam\", target_label = 0):\n",
    "    target_layers = cam_layer\n",
    "    \n",
    "    input_tensor = image\n",
    "\n",
    "    targets = [ClassifierOutputTarget(target_label)]\n",
    "\n",
    "    def denormalize(image):\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to(device).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to(device).view(3, 1, 1)\n",
    "\n",
    "        image = image * std + mean\n",
    "        return image\n",
    "\n",
    "    def to_numpy_uint8(image):\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        image = np.clip(image, 0, 1)\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "\n",
    "    image_to_show = denormalize(image[index])\n",
    "    image_to_show = to_numpy_uint8(image_to_show)\n",
    "    image_to_show = image_to_show / 255\n",
    "\n",
    "    match cam_type.lower():\n",
    "            case \"gradcam\":\n",
    "                with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case \"hirescam\":\n",
    "                with HiResCAM(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case \"gradcamelementwise\":\n",
    "                with GradCAMElementWise(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case \"gradcam++\":\n",
    "                with GradCAMPlusPlus(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case \"xgradcam\":\n",
    "                with XGradCAM(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case \"ablationcam\":\n",
    "                with AblationCAM(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case \"scorecam\":\n",
    "                with ScoreCAM(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case \"eigencam\":\n",
    "                with EigenCAM(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case \"eigengradcam\":\n",
    "                with EigenGradCAM(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case \"layercam\":\n",
    "                with LayerCAM(model=model, target_layers=target_layers) as cam:\n",
    "                    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            case _:\n",
    "                raise ValueError(f\"CAM type '{cam_type}' invalid\")\n",
    "    \n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    return show_cam_on_image(image_to_show, grayscale_cam, use_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = resnet50(pretrained=True)\n",
    "model_resnet.fc = nn.Linear(model_resnet.fc.in_features, 4)  # Adjusting for 4 classes\n",
    "model_resnet = model_resnet.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_resnet.parameters(), lr=1e-4)\n",
    "\n",
    "# Training configuration\n",
    "resnet_epochs = 20\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(resnet_epochs):\n",
    "    model_resnet.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, masks, filenames in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{resnet_epochs}', leave=False):\n",
    "        # Move images and masks to device\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Generate labels based on filenames\n",
    "        labels = []\n",
    "        for filename in filenames:\n",
    "            if re.search(r\"healthy\", filename):\n",
    "                labels.append(0)\n",
    "            elif re.search(r\"mild\", filename):\n",
    "                labels.append(1)\n",
    "            elif re.search(r\"moderate\", filename):\n",
    "                labels.append(2)\n",
    "            elif re.search(r\"severe\", filename):\n",
    "                labels.append(3)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "        # Zero the gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_resnet(images)  # Raw logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training loss\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Calculate predictions and accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)  # Get predicted class\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    # Compute epoch loss and accuracy\n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    # Print epoch metrics\n",
    "    print(f'Epoch {epoch + 1}/{resnet_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, resnet_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, resnet_epochs + 1), train_accuracies, color='green', label='Training Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "image = []\n",
    "mask = []\n",
    "\n",
    "model_resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks, filenames in tqdm(train_dataloader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        if torch.any(masks > 0):\n",
    "            test_image_resnet = images[0].detach()\n",
    "            image = test_image_resnet\n",
    "            mask = masks[0].detach()\n",
    "            break\n",
    "\n",
    "first_image_batch = test_image_resnet.unsqueeze(0)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cam_image = generate_ResNet_CAM(model_resnet, first_image_batch, 0, [model_resnet.layer3[-1]], \"xgradcam\")\n",
    "plt.imshow(cam_image)\n",
    "plt.axis('off')\n",
    "plt.title('Grad-CAM')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "def denormalize(image):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image\n",
    "\n",
    "image_to_show = denormalize(image)\n",
    "plt.imshow(image_to_show)\n",
    "plt.imshow(mask.cpu().numpy()[0], alpha=0.5, cmap='plasma')\n",
    "plt.axis('off')\n",
    "plt.title('Image with Mask Overlay')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 3\n",
    "classes = 1\n",
    "model = UNet(channels, classes, use_attention=use_attention, attention_type=attention_model, att_4 = False)\n",
    "input_data = torch.randn(1, channels, width, height)\n",
    "output = model(input_data)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = epochs\n",
    "\n",
    "training_accuracies = []\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "# Loading and saving\n",
    "checkpoint_path = \"model_checkpoint\" + attention_model + \".pth\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    training_accuracies = checkpoint['training_accuracies']\n",
    "    training_losses = checkpoint['training_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    val_accuracies = checkpoint['val_accuracies']\n",
    "    print(f\"Checkpoint loaded. Resuming training from epoch {start_epoch}.\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, training_accuracies, training_losses, val_losses, val_accuracies):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'training_accuracies': training_accuracies,\n",
    "        'training_losses': training_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "\n",
    "\n",
    "images_dir = \"epoch_images\" + attention_model\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "for images, masks , filenames in test_dataloader:\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "    if torch.any(masks > 0):\n",
    "        first_image_in_dataset = images[0].detach()\n",
    "        first_mask_in_dataset = masks[0].detach()\n",
    "        break\n",
    "first_image_batch = first_image_in_dataset.unsqueeze(0)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "\n",
    "    for images, masks, filenames in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = F.binary_cross_entropy(outputs, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #thresholding the prediction values\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        binary_predictions = (outputs > 0.5).float()\n",
    "        \n",
    "        #calculating the correct pixels\n",
    "        correct_predictions += (binary_predictions == masks).sum().item()\n",
    "\n",
    "        total_samples += masks.numel()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    training_accuracies.append(epoch_accuracy)\n",
    "    training_losses.append(epoch_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy * 100:.2f}%\")\n",
    "\n",
    "    #validation\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0   \n",
    "    with torch.no_grad():\n",
    "        for val_images, val_masks, filenames in tqdm(test_dataloader, desc='Validation', leave=False):\n",
    "            val_images, val_masks = val_images.to(device), val_masks.to(device)\n",
    "            val_outputs = model(val_images)\n",
    "\n",
    "            val_loss += F.binary_cross_entropy(val_outputs, val_masks).item()\n",
    "\n",
    "            val_binary_predictions = (val_outputs > 0.5).float()\n",
    "\n",
    "            val_correct_predictions += (val_binary_predictions == val_masks).sum().item()\n",
    "\n",
    "            val_total_samples += val_masks.numel()\n",
    "\n",
    "    val_epoch_loss = val_loss / len(test_dataloader)\n",
    "    val_epoch_accuracy = val_correct_predictions / val_total_samples\n",
    "\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy * 100:.2f}%\\n\")\n",
    "    print(\"============================================\")\n",
    "            \n",
    "\n",
    "    if(show_images):\n",
    "        heatmap = generate_heatmap_with_tensor(first_image_in_dataset.to(device).half())#making heatmap only of the first image of the batch\n",
    "        resnet_img = generate_ResNet_CAM(model_resnet, first_image_batch, 0, [model_resnet.layer3[-1]], \"xgradcam\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(first_image_batch)\n",
    "            probs = outputs\n",
    "            images_np = first_image_in_dataset.cpu().numpy().transpose((1, 2, 0))\n",
    "            masks_np = first_mask_in_dataset.cpu().numpy().transpose((1, 2, 0))\n",
    "            probs_np = probs.cpu().numpy()\n",
    "            threshold = 0.5\n",
    "            preds_np = (probs_np > threshold).astype(np.uint8)\n",
    "            preds_np_squeezed = np.squeeze(preds_np, axis=1) \n",
    "            plt.figure(figsize=(16, 8))\n",
    "            plt.subplot(1, 4, 1)\n",
    "            plt.imshow(images_np)\n",
    "            plt.imshow(masks_np, alpha=0.5, cmap='plasma')\n",
    "            plt.title(\"Original Image with Ground Truth Mask\")\n",
    "            plt.axis('off')\n",
    "            plt.subplot(1, 4, 2)\n",
    "            plt.imshow(images_np)\n",
    "            plt.imshow(preds_np_squeezed[0], alpha=0.5, cmap='plasma')\n",
    "            plt.title(\"Predicted Mask\" + f\" ({val_epoch_accuracy * 100:.2f}%)\" + f\"Epoch {epoch + 1}\")\n",
    "            plt.axis('off')\n",
    "                    \n",
    "            #gradcam heatmap\n",
    "            img = first_image_in_dataset.cpu().detach().permute(1, 2, 0).numpy()\n",
    "            img = (img - img.min()) / (img.max() - img.min()) #normalizing\n",
    "            img = np.uint8(255 * img)\n",
    "\n",
    "            heatmap = np.uint8(255 * heatmap)\n",
    "            heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "            heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "            superimposed_img = cv2.addWeighted(heatmap, 0.6, img, 1 - 0.6, 0)\n",
    "\n",
    "            plt.subplot(1, 4, 3)\n",
    "            plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title('GradCAM with Overlay')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(1, 4, 4)\n",
    "            plt.imshow(resnet_img)\n",
    "            plt.title('GradCAM ResNet')\n",
    "            plt.axis('off')\n",
    "\n",
    "            #saving the figs for the gif\n",
    "            plt.savefig(os.path.join(images_dir, f'epoch_{epoch + 1}.png'))\n",
    "            if epoch % 5 == 0: \n",
    "                plt.show()\n",
    "            plt.close()\n",
    "    if (epoch % new_aug_epoch_distance == 0 and epoch != 0) | epoch == num_epochs - 1: #dynamic data loader changing to improve loss\n",
    "        train_dataloader, test_dataloader = recreate_data_loader(number_of_augmentations)\n",
    "        save_checkpoint(epoch, model, optimizer, training_accuracies, training_losses, val_losses, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    images.append(imageio.v2.imread(os.path.join(images_dir, f'epoch_{epoch}.png')))\n",
    "\n",
    "gif_path = os.path.join(images_dir, 'training_progress.gif')\n",
    "imageio.mimsave(gif_path, images, duration=1, loop=0)\n",
    "\n",
    "display.Image(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "- **Accuracy**: The number of correct predictions out of the total of predictions. Higher means better.\n",
    "\n",
    "- **Loss**: How well the predictions matched the expected result. Lower means better.\n",
    "\n",
    "- **Dice**: Calculates the similarity between the intersection of two images. Higher means better.\n",
    "\n",
    "- **Intersection over Union (IoU)**: It calculates the overlap between the predicted image and the ground truth, but it also takes in consideration the union of the boxes, while Dice only accounts for the intersection.Higher means better.\n",
    "\n",
    "- **Panoptic Quality (PQ)**: Indicates the performance in segmenting instances accurately. Higher means better.\n",
    "\n",
    "- **Aggregated Jaccard Index (AJI)**: Indicate the performance of the alignment between predicted and ground truth instance boundaries. Higher means better.\n",
    "\n",
    "- **Specificity**: Measures the proportion of actual negatives that are correctly identified. Higher means better.\n",
    "\n",
    "- **Sensitivity**: Measures the proportion of actual positives that are correctly identified. Higher means better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(predictions, targets):\n",
    "    true_positives = ((predictions == 1) & (targets == 1)).sum().item()\n",
    "    true_negatives = ((predictions == 0) & (targets == 0)).sum().item()\n",
    "    false_positives = ((predictions == 1) & (targets == 0)).sum().item()\n",
    "    false_negatives = ((predictions == 0) & (targets == 1)).sum().item()\n",
    "    \n",
    "    intersection = torch.logical_and(predictions, targets).sum().item()\n",
    "    union = torch.logical_or(predictions, targets).sum().item()\n",
    "    \n",
    "    return true_positives, true_negatives, false_positives, false_negatives, intersection, union\n",
    "\n",
    "model.eval()\n",
    "accuracies = []\n",
    "losses = []\n",
    "dices = [] \n",
    "ious = []\n",
    "specificities = []\n",
    "sensitivities = []\n",
    "pqs = []\n",
    "ajis = []\n",
    "\n",
    "ev_running_loss = 0.0\n",
    "ev_correct_predictions = 0\n",
    "ev_total_samples = 0\n",
    "ev_true_positives = 0\n",
    "ev_true_negatives = 0\n",
    "ev_false_positives = 0\n",
    "ev_false_negatives = 0\n",
    "ev_aji = 0\n",
    "batch_iou = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ev_images, ev_masks, _ in test_dataloader:\n",
    "        ev_images, ev_masks = ev_images.to(device), ev_masks.to(device)\n",
    "        ev_outputs = model(ev_images)\n",
    "        ev_loss = F.binary_cross_entropy(ev_outputs, ev_masks)\n",
    "        ev_running_loss += ev_loss.item()\n",
    "        losses.append(ev_loss.item())\n",
    "\n",
    "        #Thresholding the prediction values\n",
    "        ev_binary_predictions = (ev_outputs > 0.5).float()\n",
    "\n",
    "        #Calculating the correct pixels\n",
    "        ev_correct_predictions += (ev_binary_predictions == ev_masks).sum().item()\n",
    "        ev_total_samples += ev_masks.numel()\n",
    "\n",
    "        #Calculate metrics\n",
    "        tp, tn, fp, fn, intersection, union = calculate_metrics(ev_binary_predictions, ev_masks)\n",
    "        \n",
    "        ev_true_positives += tp\n",
    "        ev_true_negatives += tn\n",
    "        ev_false_positives += fp\n",
    "        ev_false_negatives += fn\n",
    "        \n",
    "        #Calculating IoU\n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "        batch_iou += iou\n",
    "        ious.append(iou)\n",
    "\n",
    "        #Calculating AJI\n",
    "        aji_numerator = intersection\n",
    "        aji_denominator = union + fp + fn\n",
    "        aji = aji_numerator / aji_denominator if aji_denominator > 0 else 0.0\n",
    "        ev_aji += aji\n",
    "        ajis.append(aji)\n",
    "\n",
    "        #Calculate accuracy\n",
    "        batch_accuracy = (ev_binary_predictions == ev_masks).sum().item() / ev_masks.numel()\n",
    "        accuracies.append(batch_accuracy)\n",
    "\n",
    "        #Calculate Dice coefficient\n",
    "        dice_coefficient = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0\n",
    "        dices.append(dice_coefficient)\n",
    "\n",
    "        #Precision and recall\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        pqs.append(precision * recall if precision + recall > 0 else 0.0)\n",
    "        \n",
    "        #Specificity and sensitivity\n",
    "        sensitivities.append(recall)\n",
    "        specificities.append(tn / (tn + fp) if (tn + fp) > 0 else 0.0)\n",
    "\n",
    "#Precision and recall\n",
    "precision = ev_true_positives / (ev_true_positives + ev_false_positives) if (ev_true_positives + ev_false_positives) > 0 else 0.0\n",
    "recall = ev_true_positives / (ev_true_positives + ev_false_negatives) if (ev_true_positives + ev_false_negatives) > 0 else 0.0\n",
    "\n",
    "#Specificity and sensitivity\n",
    "ev_sensitivity = recall\n",
    "ev_specificity = ev_true_negatives / (ev_true_negatives + ev_false_positives) if (ev_true_negatives + ev_false_positives) > 0 else 0.0\n",
    "\n",
    "#Evaluation loss and accuracy\n",
    "ev_loss = ev_running_loss / len(test_dataloader)\n",
    "ev_accuracy = ev_correct_predictions / ev_total_samples\n",
    "\n",
    "#Dice coefficient\n",
    "ev_temp_dice = (2 * ev_true_positives) / (2 * ev_true_positives + ev_false_positives + ev_false_negatives) if (2 * ev_true_positives + ev_false_positives + ev_false_negatives) > 0 else 0.0\n",
    "\n",
    "#PQ\n",
    "ev_pq = precision * recall if precision + recall > 0 else 0.0\n",
    "\n",
    "#IoU\n",
    "iou = batch_iou / len(test_dataloader)\n",
    "\n",
    "#AJI\n",
    "aji = ev_aji / len(test_dataloader)\n",
    "\n",
    "#Calculate standard deviation\n",
    "loss_std = np.std(losses)\n",
    "accuracy_std = np.std(accuracies)\n",
    "dice_std = np.std(dices)\n",
    "iou_std = np.std(ious)\n",
    "specificity_std = np.std(specificities)\n",
    "sensitivity_std = np.std(sensitivities)\n",
    "pq_std = np.std(pqs)\n",
    "aji_std = np.std(ajis)\n",
    "\n",
    "print(f'Evaluation Loss: {ev_loss * 100:0.2f}% (Std: {loss_std * 100:0.2f}%)')\n",
    "print(f'Evaluation Accuracy: {ev_accuracy * 100:0.2f}% (Std: {accuracy_std * 100:0.2f}%)')\n",
    "print(f'Dice Coefficient: {ev_temp_dice * 100:0.2f}% (Std: {dice_std * 100:0.2f}%)')\n",
    "print(f'IoU: {iou * 100:0.2f}% (Std: {iou_std * 100:0.2f}%)')\n",
    "print(f'Specificity: {ev_specificity * 100:0.2f}% (Std: {specificity_std * 100:0.2f}%)')\n",
    "print(f'Sensitivity: {ev_sensitivity * 100:0.2f}% (Std: {sensitivity_std * 100:0.2f}%)')\n",
    "print(f'PQ: {ev_pq * 100:0.2f}% (Std: {pq_std * 100:0.2f}%)')\n",
    "print(f'AJI: {aji * 100:0.2f}% (Std: {aji_std * 100:0.2f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet predicting with model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "true_positives = [0] * num_classes\n",
    "true_negatives = [0] * num_classes\n",
    "false_positives = [0] * num_classes\n",
    "false_negatives = [0] * num_classes\n",
    "\n",
    "total_loss = 0\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for ev_images, ev_masks, ev_names in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        model_resnet.eval()\n",
    "\n",
    "        ev_images, ev_masks = ev_images.to(device), ev_masks.to(device)\n",
    "        ev_outputs = model_resnet(ev_images)\n",
    "\n",
    "        labels = []\n",
    "        for filename in ev_names:\n",
    "            if re.search(r\"healthy\", filename):\n",
    "                labels.append(0)\n",
    "            elif re.search(r\"mild\", filename):\n",
    "                labels.append(1)\n",
    "            elif re.search(r\"moderate\", filename):\n",
    "                labels.append(2)\n",
    "            elif re.search(r\"severe\", filename):\n",
    "                labels.append(3)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "        preds = torch.argmax(ev_outputs, dim=1)\n",
    "\n",
    "        loss = criterion(ev_outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        for cls in range(num_classes):\n",
    "            true_positives[cls] += ((preds == cls) & (labels == cls)).sum().item()\n",
    "            true_negatives[cls] += ((preds != cls) & (labels != cls)).sum().item()\n",
    "            false_positives[cls] += ((preds == cls) & (labels != cls)).sum().item()\n",
    "            false_negatives[cls] += ((preds != cls) & (labels == cls)).sum().item()\n",
    "\n",
    "average_loss = total_loss / len(test_dataloader)\n",
    "\n",
    "print(\"Metrics for each class:\")\n",
    "for cls in range(num_classes):\n",
    "    print(f\"\\n======= Class {cls}: =======\")\n",
    "    print(f\"True Positives: {true_positives[cls]}\")\n",
    "    print(f\"True Negatives: {true_negatives[cls]}\")\n",
    "    print(f\"False Positives: {false_positives[cls]}\")\n",
    "    print(f\"False Negatives: {false_negatives[cls]}\")\n",
    "\n",
    "    print(\"\\nOverall metrics:\")\n",
    "    accuracy = (true_positives[cls] + true_negatives[cls]) / (true_positives[cls] + false_negatives[cls] + false_positives[cls] + true_negatives[cls])\n",
    "    sensitivity = true_positives[cls] / (true_positives[cls] + false_negatives[cls])\n",
    "    specificity = true_negatives[cls] / (true_negatives[cls] + false_positives[cls])\n",
    "    precision = true_positives[cls] / (true_positives[cls] + false_positives[cls])\n",
    "    negative_predictive_value = true_negatives[cls] / (true_negatives[cls] + false_negatives[cls])\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Loss: {average_loss:.2f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Negative Predictive Value: {negative_predictive_value:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model_resnet.eval()\n",
    "counter = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for ev_images, ev_masks, ev_names in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        model_resnet.eval()\n",
    "        ev_images, ev_masks = ev_images.to(device), ev_masks.to(device)\n",
    "        ev_outputs = model(ev_images)\n",
    "        ev_binary_predictions = (ev_outputs > 0.5).float()\n",
    "        \n",
    "        new_image = ev_images[0].clone()\n",
    "        binary_mask = ev_binary_predictions[0].expand_as(new_image)\n",
    "        new_image[binary_mask == 0] = 0 # new_image[binary_mask == 0]/2\n",
    "        new_image_np = new_image.to(\"cpu\").numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "        real_class = ev_names[0]\n",
    "        file_name_without_ext = os.path.splitext(real_class)[0]\n",
    "        middle_element = file_name_without_ext.split('_')[1]\n",
    "\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.subplot(1, 5, 1)\n",
    "        plt.imshow(new_image_np)\n",
    "        plt.title(f'Predicted Mask {middle_element}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        resnet_output = model_resnet(new_image.unsqueeze(0).to(device))\n",
    "        resnet_probs = torch.nn.functional.softmax(resnet_output, dim=1)\n",
    "        predictions = resnet_probs.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        threshold = 0.30\n",
    "        class_names = [\"Healthy\", \"Mild\", \"Moderate\", \"Severe\"]\n",
    "        active_classes = [f\"{class_name}: {prediction * 100:.2f}%\" for class_name, prediction in zip(class_names, predictions) if prediction > threshold]\n",
    "        print(f\"Active Classes: {active_classes}\")\n",
    "\n",
    "        new_image_4d = new_image.unsqueeze(0)\n",
    "    total_predictions += 1\n",
    "    \n",
    "    if middle_element == \"healthy\":\n",
    "        true_answear = 0\n",
    "    elif middle_element == \"mild\":\n",
    "        true_answear = 1\n",
    "    elif middle_element == \"moderate\":\n",
    "        true_answear = 2\n",
    "    elif middle_element == \"severe\":\n",
    "        true_answear = 3\n",
    "    else:\n",
    "        true_answear = 5\n",
    "\n",
    "    if np.argmax(predictions) == true_answear:\n",
    "        correct_predictions += 1\n",
    "    print(f\"Correct answers: {correct_predictions / total_predictions * 100}%\")\n",
    "\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        plt.subplot(1, 5, idx + 2)\n",
    "        plt.imshow(generate_ResNet_CAM(model_resnet, new_image_4d, 0, [model_resnet.layer3[-1]], \"xgradcam\", target_label=idx))\n",
    "        plt.title(f'XGradcam {class_name} {(predictions[idx] * 100):.2f}%')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    if counter == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Charts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "def to_cpu_and_numpy(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.cpu().numpy()\n",
    "    return data\n",
    "\n",
    "training_losses = [to_cpu_and_numpy(loss) for loss in training_losses]\n",
    "val_losses = [to_cpu_and_numpy(loss) for loss in val_losses]\n",
    "training_accuracies = [to_cpu_and_numpy(acc) for acc in training_accuracies]\n",
    "val_accuracies = [to_cpu_and_numpy(acc) for acc in val_accuracies]\n",
    "\n",
    "plt.plot(epochs_list, training_losses, label='Training Loss', color='blue')\n",
    "plt.plot(epochs_list, val_losses, label='Validation Loss', color='orange')\n",
    "plt.plot(epochs_list, training_accuracies, label='Training Accuracy', color='green')\n",
    "plt.plot(epochs_list, val_accuracies, label='Validation Accuracy', color='red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Training and Validation Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_images:\n",
    "    counter = 0\n",
    "    brightness = 0.5\n",
    "    for images, masks, _ in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            images_normalized = images\n",
    "\n",
    "            outputs = model(images_normalized)\n",
    "            probs = outputs\n",
    "            #probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            images_np = images_normalized.cpu().numpy()\n",
    "            masks_np = masks.cpu().numpy()\n",
    "            probs_np = probs.cpu().numpy()\n",
    "\n",
    "            threshold = 0.5\n",
    "            preds_np = (probs_np > threshold).astype(np.uint8)\n",
    "            preds_np_squeezed = np.squeeze(preds_np, axis=1) \n",
    "\n",
    "            original_image = images_np[0].transpose((1, 2, 0))\n",
    "            ground_truth_mask = masks_np[0, 0]\n",
    "            predicted_mask = preds_np_squeezed[0]\n",
    "            probability_map = probs_np[0, 0]\n",
    "\n",
    "            overlay_gt = original_image.copy()\n",
    "            overlay_gt = original_image * brightness # to make it a little darker\n",
    "            overlay_gt[ground_truth_mask == 1] = [1, 1, 0]\n",
    "\n",
    "            overlay_pred = original_image.copy()\n",
    "            overlay_pred = original_image * brightness\n",
    "            overlay_pred[predicted_mask == 1] = [0, 1, 1]\n",
    "\n",
    "            overlay_diff = original_image.copy()\n",
    "            overlay_diff = original_image * brightness \n",
    "            overlay_diff[(ground_truth_mask == 1) & (predicted_mask == 1)] = [0, 1, 0]  #correct predictions\n",
    "            overlay_diff[(ground_truth_mask == 1) & (predicted_mask == 0)] = [1, 0, 0]  #missed predictions\n",
    "            overlay_diff[(ground_truth_mask == 0) & (predicted_mask == 1)] = [0, 0, 1]  #false positives\n",
    "\n",
    "            plt.figure(figsize=(28, 4))\n",
    "\n",
    "            plt.subplot(1, 7, 1)\n",
    "            plt.imshow(original_image)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Original Image\")\n",
    "\n",
    "            plt.subplot(1, 7, 2)\n",
    "            plt.imshow(ground_truth_mask, cmap='gray')\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Ground Truth Mask\")\n",
    "\n",
    "            plt.subplot(1, 7, 3)\n",
    "            plt.imshow(probability_map, cmap='plasma')\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Probability Map\")\n",
    "\n",
    "            plt.subplot(1, 7, 4)\n",
    "            plt.imshow(predicted_mask, cmap='gray')\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Predicted Mask\")\n",
    "\n",
    "            plt.subplot(1, 7, 5)\n",
    "            plt.imshow(overlay_gt)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Overlay Ground Truth\")\n",
    "\n",
    "            plt.subplot(1, 7, 6)\n",
    "            plt.imshow(overlay_pred)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Overlay Pred\")\n",
    "\n",
    "            green_patch = mpatches.Patch(color='green', label='True Positive')\n",
    "            red_patch = mpatches.Patch(color='red', label='False Negative')\n",
    "            blue_patch = mpatches.Patch(color='blue', label='False Positive')\n",
    "            plt.subplot(1, 7, 7)\n",
    "            plt.imshow(overlay_diff)\n",
    "            plt.title(\"Overlay Diff\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.legend(handles=[green_patch, red_patch, blue_patch], loc='upper right')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        #showing gradcams\n",
    "        resnet_img = generate_ResNet_CAM(model_resnet, images, 0, [model_resnet.layer3[-1]], \"xgradcam\")\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(resnet_img)\n",
    "        plt.title(\"ResNet GradCam\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        #gradcam heatmap\n",
    "        heatmap = generate_heatmap_with_tensor(images[0].half())\n",
    "        img = images[0].cpu().detach().permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min()) #normalizing\n",
    "        img = np.uint8(255 * img)\n",
    "        heatmap = np.uint8(255 * heatmap)\n",
    "        heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "        superimposed_img = cv2.addWeighted(heatmap, 0.6, img, 1 - 0.6, 0)\n",
    "        plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('GradCAM with Overlay')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        counter += 1\n",
    "        if counter == 10:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
